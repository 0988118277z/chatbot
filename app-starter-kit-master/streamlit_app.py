import streamlit as st
from langchain_openai import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import pymongo, json, os, jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
from itertools import islice

api_key = os.getenv('openai_api_key')
user = os.getenv('mongodb_user')
passwd = os.getenv('mongodb_password')
host = os.getenv('mongodb_ip')
port = os.getenv('mongodb_port')
db = os.getenv('mongodb_db')

# App title
st.set_page_config(page_title="ðŸ¤—ðŸ’¬ HugChat")

st.markdown(
    r"""
    <style>
    .stDeployButton {visibility: hidden;}
    p {color: red;}
    </style>
    """, unsafe_allow_html=True
)

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", 
        "content": "æˆ‘æ˜¯å…«å¦æ©Ÿå™¨äººï¼Œå‘Šè¨´æˆ‘ä½ æƒ³çŸ¥é“çš„å…«å¦ä¸»é¡Œï¼Œæˆ‘å¯ä»¥å‘Šè¨´ä½ å…¶ä»–äººè¨Žè«–çš„å…«å¦å…§å®¹ï¼Œç¯„ä¾‹ï¼šæˆ‘æƒ³çŸ¥é“å¯Œå£«å±±çš„å…«å¦"}]

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# world process
def preprocess_text_chinese(text):
    import jieba
    stopwords = [".", "=", ",", "ï¼Œ", "ã€‚", "?", "ï¼", "ï¼Ÿ", "æŽ¨", "äº†", "å•¦", "çš„", "é˜¿", "å•Š", "å§", "å›‰", "å–”", "å—Ž", "å“ˆ"]
    if 'http' not in text:
        for i in stopwords:
            text = text.replace(i,'')
        text = ' '.join(jieba.cut(text))
        return text
    return ''

# Function for generating LLM response
def generate_response(prompt_txt):
    llm = OpenAI(api_key=api_key, temperature=0.1)

    mongo_dbs = pymongo.MongoClient(f"mongodb://{user}:{passwd}@{host}:{port}/{db}")
    mydb = mongo_dbs["pttdata"]
    mycollection = mydb["messages"]

    prompt_template = PromptTemplate(input_variables=["user_input"],template="""
        '_id': å…«å¦è³‡æ–™ç·¨è™Ÿ,
        'gossiping_user': å…«å¦ç™¼èµ·äºº,
        'gossiping_title': å…«å¦æ¨™é¡Œ,
        'gossiping_time': å…«å¦ç™¼èµ·æ™‚é–“,
        'gossiping_content': å…«å¦èªªæ˜Ž,
        'message': ['user': è¨Žè«–è€…åç¨±, 'content': è¨Žè«–å…§å®¹,'user': è¨Žè«–è€…åç¨±, 'content': è¨Žè«–å…§å®¹,'user': è¨Žè«–è€…åç¨±, 'content': è¨Žè«–å…§å®¹]
        
        é€™æ˜¯æˆ‘çš„mongodbçµæ§‹ï¼Œè«‹å…ˆåˆ†æž "{user_input}" é€™å€‹å°è©±ï¼Œåˆ†è¾¨ä½¿ç”¨è€…æ˜¯æƒ³äº†è§£çš„æ˜¯ä¸»é¡Œé¡žåž‹é‚„æ˜¯å…§å®¹é¡žåž‹ï¼Œ                         
        å¦‚æžœä½ åˆ†è¾¨å‡ºçš„æ˜¯ä¸»é¡Œé¡žåž‹ï¼Œè«‹å¾žgossiping_titleå›žå¾©mongodbèªžæ³•ï¼Œç¯„ä¾‹: è¼¸å…¥:keywordçš„å…«å¦  è¼¸å‡º:{{"gossiping_content":{{"$regex":"keyword"}}}},{{"gossiping_title":1,"message.content":1,"_id":0}}
        å¦‚æžœä½ åˆ†è¾¨å‡ºçš„æ˜¯å…§å®¹é¡žåž‹ï¼Œè«‹å¾žgossiping_titleå›žå¾©mongodbèªžæ³•ï¼Œç¯„ä¾‹: è¼¸å…¥:ä¸‹é›¨å½±éŸ¿åˆ°keywordçš„å…«å¦å…§å®¹  è¼¸å‡º:{{"gossiping_title":{{"$regex":"keyword"}}}},{{"gossiping_title":1,"message.content":1,"_id":0}}
        æˆ‘è¦å°‡ä½ çš„å›žè¦†å…§å®¹ä½œç‚ºè‡ªå‹•æŸ¥è©¢è³‡æ–™åº«çš„æŒ‡ä»¤ï¼Œæ‰€ä»¥å›žè¦†å…§å®¹ä¸è¦æœ‰ä»»ä½•èªªæ˜Žï¼Œä¸è¦æœ‰ä»»ä½•ä¸­æ–‡å­—ã€‚
        """)
    chain = LLMChain(llm=llm, prompt=prompt_template)
    result = chain.run(user_input=prompt_txt)

    try:
        mongo_query = result.split('è¼¸å‡º:')[1].replace(' ','')
    except:
        mongo_query = result
     
    separator_index = mongo_query.find('},') + 1
    query_str = mongo_query[:separator_index]
    projection_str = mongo_query[separator_index + 1:]

    query = json.loads(query_str)
    projection = json.loads(projection_str)
    
    query_result = mycollection.find(query,projection)
    mongo_data = [ i for i in query_result]
    mongo_dbs.close()

    mongo_data_list = {i['gossiping_title']:[j['content'] for j in i['message']] for i in mongo_data}
    
    if len(mongo_data_list) > 5:
        key_list = [ preprocess_text_chinese(keys) for keys, values in mongo_data_list.items()]

        # TF-IDF
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(key_list)
        
        cosine_sim_matrix = cosine_similarity(tfidf_matrix)
        cosine_dist_matrix = 1 - cosine_sim_matrix
        cosine_dist_matrix[cosine_dist_matrix < 0] = 0  # å°‡æ‰€æœ‰å°æ–¼0çš„è®Š0
        # dbscan
        dbscan = DBSCAN(eps=0.4, min_samples=1, metric='precomputed')
        dbscan.fit(cosine_dist_matrix)
        
        cluster_labels = dbscan.labels_
        clustered_data = {}

        for i, label in enumerate(cluster_labels):
            if label not in clustered_data:
                clustered_data[label] = []
            clustered_data[label].append(list(mongo_data_list.keys())[i])      
        
        return_keyword = 'ç”±æ–¼é¡žåž‹éŽå¤šï¼Œè«‹å¾žé¸æ“‡ä¸€å€‹ä½ æƒ³è¦çŸ¥é“çš„å…§å®¹\n'
        # print(f'é¸æ“‡ä¸€å€‹ä½ æƒ³è¦çŸ¥é“çš„å…§å®¹')
        if len(clustered_data) > 100:  #é¿å…é¡¯ç¤ºçµæžœå¤ªå¤š
            start_index = max(0, len(clustered_data) - 100)
            clustered_data = dict(islice(clustered_data.items(), start_index, None))
        for keys, values in clustered_data.items():
            try:
                return_keyword += f"{int(keys) + 1}:{values[0].split(']')[1]}"
                # print(keys,':',values[0].split(']')[1])
            except:
                return_keyword += f"{int(keys) + 1}:{values[0]}"
                # print(keys,':',values[0])
            return_keyword += '  \n'  #å¤šè¡Œç”¨æ›è¡Œé¡¯ç¤º
        return return_keyword
        
    message_list = [] #éŽæ¿¾ç©ºç™½çš„ç•™è¨€ 
    for key, values in mongo_data_list.items():
        for i in values:
            if preprocess_text_chinese(i) != ' ' and i != '':
                message_list.append(preprocess_text_chinese(i))
    if len(message_list) > 0:
        # TF-IDF
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(message_list)
        
        #é¤˜é–’è¨ˆç®—
        cosine_sim = cosine_similarity(tfidf_matrix)
        unique_message = []
        for i in range(len(message_list)):
            if all(cosine_sim[i, j] < 0.5 for j in range(len(message_list)) if i != j):    #ç›¸é—œçš„ç•™è¨€ä¸è¦
                unique_message.append(message_list[i].replace(' ',''))
    else:
        unique_message = None
                
    # print(len(str(unique_message)))
    llm2 = OpenAI(api_key=api_key, temperature=0.8, max_tokens=2048)
    prompt_record = PromptTemplate(input_variables=["user_input","mongo_records"],template="""
        "{mongo_records}"ï¼Œé€™äº›æ˜¯è³‡æ–™ï¼Œè«‹åˆ†æžé€™äº›è³‡æ–™ï¼Œç„¶å¾Œå¾ž "{user_input}" çš„ä¸»é¡Œï¼Œå›žè¦†å…§å®¹ã€‚
        """)
    end_chain = LLMChain(llm=llm2, prompt=prompt_record)
    result = end_chain.run(user_input=prompt_txt, mongo_records=unique_message)
    return result
    

# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response(prompt)
            st.write(response) 
    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)